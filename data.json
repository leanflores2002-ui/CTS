{
  "meta": {
    "heroImage": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/transparency-ai.jpg"
  },
  "topics": [
    {
      "emoji": "âš–ï¸",
      "title": "Sesgo AlgorÃ­tmico",
      "description": "CÃ³mo los modelos pueden amplificar prejuicios presentes en los datos y quÃ© medidas aplicar.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/bias-algorithms.jpg"
    },
    {
      "emoji": "ğŸ”’",
      "title": "Privacidad",
      "description": "ProtecciÃ³n de datos personales y principios para diseÃ±ar sistemas respetuosos con la privacidad.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/privacy-data.jpg"
    },
    {
      "emoji": "ğŸ”",
      "title": "Transparencia",
      "description": "Entender y comunicar cÃ³mo funcionan las decisiones automÃ¡ticas.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/transparency-ai.jpg"
    },
    {
      "emoji": "ğŸ‘¥",
      "title": "Responsabilidad",
      "description": "QuiÃ©n responde cuando la IA causa daÃ±o y cÃ³mo establecer auditorÃ­as.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/accountability.jpg"
    },
    {
      "emoji": "ğŸŒ",
      "title": "Impacto Social",
      "description": "Efectos en empleo, desigualdad y polÃ­ticas pÃºblicas.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/social-impact.jpg"
    }
  ],
  "cases": [
    {
      "title": "Amazon Recruiting â€” sesgo en selecciÃ³n",
      "description": "Un sistema entrenado con CV histÃ³ricos reprodujo sesgos de gÃ©nero. Necesidad: datasets representativos y auditorÃ­as continuas.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/amazon-face-recognition.jpg"
    },
    {
      "title": "Reconocimiento facial â€” precisiÃ³n desigual",
      "description": "Errores mÃ¡s altos en mujeres y personas de piel oscura; riesgo en aplicaciones policiales y de seguridad.",
      "image": "https://raw.githubusercontent.com/leanflores2002-ui/etica-ai-futuro/main/src/assets/facial-recognition.jpg"
    }
  ],
  "resources": [
    {
      "name": "Partnership on AI",
      "url": "https://www.partnershiponai.org",
      "description": "Buenas prÃ¡cticas y recomendaciones globales.",
      "icon": "ğŸ›ï¸"
    },
    {
      "name": "Google â€” Responsible AI",
      "url": "https://ai.google/responsibilities",
      "description": "GuÃ­as y herramientas para diseÃ±o responsable.",
      "icon": "ğŸ“š"
    },
    {
      "name": "Oxford Insights",
      "url": "https://www.oxfordinsights.com",
      "description": "InvestigaciÃ³n sobre gobernanza y polÃ­ticas pÃºblicas.",
      "icon": "ğŸ”"
    },
    {
      "name": "ITU â€” AI for Good",
      "url": "https://www.itu.int/en/ITU-D/AI/Pages/default.aspx",
      "description": "Iniciativas y estÃ¡ndares internacionales.",
      "icon": "ğŸŒ"
    }
  ]
}